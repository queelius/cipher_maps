



(1) approximate map. reformulate lower-bound on space efficiency. we assume an in-place encoding such that you can't simply compress entire contents. well, ok, this has to do with marginal entropy again.


suppose:

    H(v1 | vj) < H(v1) for some j != 1. that means H(v1,vj) < H(v1) + H(vj).

    H(v1,vj) = H(vj) + H(v1 | vj)

    however, we don't get too exploit this correlation since we'd need to compress and decompress the joint v1 vj together and we insist only decompressing v1 for a query on k1. therefore, we are exclusively interested in the MARGINAL
    entropy of the values, e.g., H(v1) not H(v1 | vj).

    if we could do the joint entropy, then suppose v1=v2=...=vn. Then, H(v1,...,vn) = H(v1) + H(v2 | v1) + ... +
        H(vn | v1, ..., v{n-1}) = H(v1).

    suppose that the joint entropy is m * mu = H(v1,...,vn). then, we compress at get m * mu bits. Then, we do pointers into decompressed values of size l = BL(v1) + ... + BL(vn) for log2(l) pointer widths where we suppose vj is self-delimiting. so, m * mu + log2(l) total size.

    

SHMAP stuff
-----------
new stuff
---------

"shorter" codes by redundancy. more common symbols map to more codewords. this could leak info in an oblivious map, but if it codes things like gaps, prolly not much is revealed. can you come up with exact formula if p(sj) is known for each symbol? (coding p(xj=sj|x{j-1}=sk) may reveal too much though).

---
multiple ways to code something.

let's create another algorithm, one for an alphabet.


let each source symbol from the alphabet
    S = {s1,s2,...,sn}
be encoded into a uniquely decodable code over an alphabet of size 2 with codeword lengths
    l1, l2, ..., ln
then
    2^-l1 + 2^-l2 + ... + 2^-ln <= 1

conversely, for a given set of natural numbers l1,l2,...,ln satisfying the above inequality, there exists a uniquely decodable code over an alphabet of size 2 with those codeword lengths.

so, if a keys map to lists of values from the alphabet s1, ..., sn, then we may find infinitely many prefix free codes for it. we are not necessarily penalized by using multiple codings (except as a complication), since that increases the probability of finding a singular hash.

less entropy, more compression -- smaller expected codeword size. however, this entails overhead by itself. so, let's consider universal coders instead, that don't require extra stuff.

every symbol maps to a codeword. for simplicity, let's use fixed-length codewords. since we have n symbols, we start at m1 = ceiling(log2(n)) codeword lengths and there are n! ways to label the alphabet S. next, we can do m2 = m1+1 codeword lengths. no doubt, not every of the n! ways will result in codings with a prefix that aren't the same as a previous one.

anyway, as a first attempt, the compressed value can be used, all of which have an average of mu per value marginal entropy. this is smallest bit length. but we try more codings if that fails for one. it's not going to be a big savings... but, the main reason for this is so that 'fake' keys can always map to a valid value without even checking what the value part of the hash is. so, those fakes just add -log2 varepsilon per key, and the values can be quite large.

let's say the first p bits of the value part of the hash is for identifying the encoding, of which there are 2^p at most. if there are actually l < 2^p, then just take 2^p mod l. there are still only p encodings, this mod thing doesn't change expected size compared to if l did equal 2^p.

what is the prob that a particular code choice was made for a value? well, given that a b_n creates a hash that maps to some coding of the list of values for a key, the probability for coding l is
    2^-l1 + ... + 2^-l2
    -------------------
    sum of all such codelenghs

so, smaller codelengths have greater probability, i.e., the max entropy coder is most likely (maybe much more likely).

maybe having multiple is just a losing battle, extra overhead (identifying code with p bits at start) cancels out gains. even then, it's still needed for the fake keys. this also creates the complication that if only fake keys get the larger coding, it's easy to see... so, the optimal code or the always-works larger code need to be chosen with some probability p Bernoulli trial. p=.5 is best for entropy, so that adversary doesn't have much of a clue upon seeing a larger coded value for a key.

if alphabet is large and don't want keys that map to seemingly impossibly large values, then you can make multiple codewords for the end one, so if j codewords for it, then probability that it occurs is j/2^|codeword length|. you can do this for many, say you want to code gaps and want some gaps to appear more often, then you its geometric j/2^j and r of them is negative binomial.

OK OK..
how about just multiple codewords per symbol? most likely symbols... maybe not coded for smaller lengths since that'd sort of give away information about other keys. just use multiple prefix free symbols? investigate.

---
pmapf -- bit string index into like before. however, can't just store gaps in bit string. hash(x + b) = h. use this h as decoder for gaps in some way, i.e., decode_gap(hash(x+b),obfuscated gap bits) = gap. to let fake keys be able to reuse bit string, the gaps coded in a way that lets it work...




======


1) SHM is optimal. Space efficiency, entropy. When values have an average entropy mu, then yeah.

Okay, so, what's the expectation come to? Graph it. Try to evaluate it. Once we figure it out, let's think about an example.

Ex:
4 keys map to 4 values. Compress the values independently. This is an estimate of the marginal entropies. Then, sum of compressed bit lengths / 4 is an estimate of mu.

Perfect hash map:

Compress the value. Yeah, worst-case, needs to be decompressed. Or, inline compression, like Huffman. Then, no decompression necessary. And a quick iteration through may be able to handle some reptitions, mostly like this with a prefix-free code.

-m log2 v + log2 m * mu + m * mu
where mu = (entropy(v1) + ... + entropy(vm))/m


-m 1.44 log2 v + log2 m + log2 mu + m * mu

log2 (m * mu / v^[1.44m]) + m * mu 

maybe do the calculation when you do a greedy search for duplicates. this may be why it pays to code bit length in separate vector. may find more chances for repeats.

so, now let's compare to optimal... once i figure it out.



let me think about secure indexes now. in a secure index, optimally, bit string that codes it is maximum entropy. si code is a bit string which looks like noise.

X ~ Ber(.5)

1/2^x(1/2)^{1-x}= 1/(2^x 2^(1-x) = 1/2

for each bit.

H(X1) + H(X2) + ... + H(Xn)
-n log2 1/2
= n log2 2 = n

make all si's same bit length. yeah, not optimal, but trade-off. now, we know that, by that one principle of adversary knowing the algorithm, he can estimate an upper bound, i.e., each si has at most "this many elements" for an approx set.

so si is not compressible ideally. so, the more space efficient, the smaller the largest index. now, make all that size, even small ones. let's say max index has m elements and approx set is used.
okay, optimal size of db of n indexes is
    -n m log2 v, v is fp rate.

phf gets
    -n 1.44 m log2 v,
so efficient is between 0 and 1 and is
    e(si,si*)=1/1.44=.7
shs is
    e() = 1

however, phf has less entropy on account of how it represents the set. entropy is now a function of load factor, which is unknown. so, the phf needs to work with the load factor. say the max si has m elements, but we want load factor to be .25 so that it's hard to tell how many elements there are. so if  we have load factor .25 and m elements, then adversary knows there are between
1 and 4m elements. maybe? well, gotta rethink this, because BL may be computed as function of load factor r, fp rate e, and members m. we know e, we don't know r and m.

r = m / N, so m = r N, we know N. so, load factor r unknown, m as a function of r. 











what about for a secure map? slightly diff. analysis.



so, entropy. perf hash map. -m log v + 1.44 m for minimal




