---
title: "Universal function Bernoulli approximators "
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Oblivious maps {#sec:map}
=========================
A set is an unordered collection of distinct elements, typically from some implicitly understood universe.
A countable set is a *finite set* or a *countably infinite set*. A *finite set* has a finite number of elements, such as $\{ 1, 3, 5 \}$, and 
a *countably infinite set* can be put in one-to-one correspondence with the set of natural numbers, $\{1,2,3,4,5,\ldots\}$.
The cardinality of a set $\mathbb{A}$, denoted by $|\mathbb{A}|$, is a measure on the number of elements in the set, e.g., $|\{a,b\}|=2$.

A map represents a *many*-to-*one* relationship. A map that associates elements in $\mathbb{X}$ to elements in $\mathbb{Y}$ has a type denoted by $\mathbb{X}\mapsto\mathbb{Y}$.
For a map of type $X \mapsto Y$, we denote $X$ the *input* and $Y$ the output.
Typically, it is relatively easy to find which output is associated with a given
input, but the inverse operation, determining which inputs are associated with a
given output is computationally harder.
Of course, this is not necessarily the case, and mathematically the map is just
a one-to-many relation over $X \times Y$.

For instance, Table \ref{tbl:tabfunc} depicts a function over a finite domain of $n$
elements, where each input $x \in X$ is associated with a single output
$y \in Y$, i.e., $y = f(x)$.

\begin{table}
    \centering
    \caption{Function $\operatorname{negate} \colon \{0,1\} \mapsto \{0,1\}$}
    \label{tbl:tabfunc}
    \begin{tabular}{|c c|} 
        \hline
        Input ($\{0,1\}$) & Output ($\{0,1\}$)\\
        \hline
        $0$ & $1$\\
        $1$ & $0$\\    
        \hline
    \end{tabular}
\end{table}

The input does not need to be a simple set like natural numbers, but rather can be any type of set, such as a set of pairs as given in \Cref{tbl:tabfun2}.

\begin{table}
\centering
\caption{Function $\operatorname{f} \colon X \mapsto Y$\\defined over a *finite* domain $\X$}
\label{tbl:tabfunc2}
\begin{tabular}{|c c|} 
\hline
Input ($\{0,1\} \times \{0,1\}$) & Output ($\{0,1\}$)\\
\hline
    $(0,0)$ & $0$\\
    $(0,1)$ & $1$\\
    $(1,0)$ & $1$\\
    $(1,1)$ & $0$\\    
\hline
\end{tabular}
\end{table}


Since we are interested in constructing maps in computer memory, we must have some way to represent them.
One technique may be given by the following table.

\begin{table}
\centering
\caption{Function $\operatorname{f} \colon \{0,1\} \mapsto \{0,1\}$ where $b : \{0,1\} \mapsto \{0,1\}^*$ is the encoder for $\mathbb{Y}$}
\label{tbl:tabfunc}
\begin{tabular}{|c c c|} 
\hline
Input ($\mathbb{X}$) & Output ($\mathbb{Y}$) & Encoding ($\cisb$)\\
\hline
    $x_1$ & $y_1$ & $b_1$\\
    $x_2$ & $y_2$ & $b_2$\\
    $x_3$ & $y_3$ & $b_3$\\
    $\vdots$ & $\vdots$ & $\vdots$\\
    $x_n$ & $y_n$ & $b_n$\\
\hline
\end{tabular}
\end{table}



The oblivious map is given by the following definition.
\begin{definition}
The *oblivious Bernoulli map* is a specialization of the Bernoulli map. We
denote an oblivious map of $f$ by $f^* = (f,\mathcal{C})$ where $\mathcal{C}$ is the
subset of the computational basis of $f$ which $f^*$ provides.
An oblivious Bernoulli map satisifes the following conditions:

- The function $f^* : X \mapsto Y$ is a Bernoulli map of $f$.
    
- If an element of $x \in X$ is not in the domain of definition,
  $f(x)$ is a random oracle over $Y$
  
- A particular mapping $y = f^*(x)$ may only be learned by applying $f^*$ to $x$.

In an *oblivious map*, a mapping (row in the table) is only learned upon request.

Observe that $f^*$ is an oblivious value. Typically, we are also interested in
functions in which the domain and codomain also represent oblivious values, i.e.,
$$
f^* : X^* \mapsto Y^*
$$
where $X^* = (X,\mathcal{C}_1)$, $Y^* = (Y,\mathcal{C}_2)$, and $f^* = (f,\mathcal{C}_3)$.

It may be the case that $X^*$ is a set of oblivious integers that, say, only
supports testing equality and addition. Of course, once we have addition, we may
also implement multiplication, powers, and many other operations.

By \cref{dummyref}, the space complexity of the Bernoulli map with an error
rate $\operatorname{error\_rate}(\hat{f}, x)$ is given by the following theorem.
\begin{theorem}
\end{theorem}

## Abstract data type
A *type* is a set and the elements of the set are called the *values* of the
type.
An *abstract data type* is a type and a set of operations on values of the type.
For example, the *integer* abstract data type is the set of all integers and a
set of standard operations (computational basis) such as addition, subtraction,
and multiplication.

A *data structure* is a particular way of organizing data and may implement one
or more abstract data types.
An *immutable* data structure has static state; once constructed, its state does
not change until it is destroyed.
Let $\hat{f} : X \mapsto Y$ model the concept of a Bernoulli approximation of
$f : X \mapsto Y$.
Then,

- $\hat{f}(x)$

  Returns a value in $Y$.

- $\operatorname{error\_rate(\hat{f},x)$.

  Returns the *error rate* of $\hat{f}$ applied to $x$, i.e.,
  $$
      \Pr\{\hat{f}(x) = f(x)} = \operatorname{error\_rate(\hat{f},x)
  $$
  for every $x \in \operatorname{dom}(f)$.







\section{The \emph{Singular Hash Map}}
The \emph{Singular Hash Set} is a theoretical data structure that provides an \emph{optimal} implementation of the \emph{oblivious map} abstract data type. A more practical implementation is given by the \emph{Perfect Map Filter}\cite{pmf}.

We consider sets in which the universe of elements is given by the \emph{countably infinite} set of all bit strings.
\begin{definition}
The countably infinite set of all bit strings is denoted by
\begin{equation}
    \cisb = \left\{b \colon b \in \left\{0,1\right\}^*\right\}\,,
\end{equation}
where $*$ denotes all non-negative integers.
\end{definition}
A \emph{finite} subset of $\cisb$ is given by the following definition.
\begin{definition}
The \emph{finite set} of all bit strings of length $n$ is denoted by
\begin{equation}
    \cisb_n = \left\{b \colon b \in \left\{0,1\right\}^n\right\}
\end{equation}
with a cardinality given by
\begin{equation}
    \card{\cisb_n} = 2^n\,.
\end{equation}
\end{definition}

A \emph{hash function} is related to countable sets $\cisb$ and $\cisb_n$ and is given by the following definition.
\begin{definition}
A \emph{hash function} $\hash \colon \cisb \mapsto \cisb_n$ is a function such that all bit strings of arbitrary-length are mapped (hashed) to bit strings of fixed-length $n$.
\end{definition}
For a given $x \in \cisb$, $y = \hash{x} \in \cisb_n$ is denoted the \emph{hash} of $x$.









The \gls{gls-shmap} depends on a random oracle as given by the following definition.
\begin{definition}
\label{def:randomoracle}
A random oracle, denoted by $\ro \colon \cisb \mapsto \cisb_n$, is a theoretical \emph{hash function} whose output is uniformly distributed over the elements of $\cisb_n$.
\end{definition}
\begin{assumption}
The hash function $\hash$ approximates a random oracle $\ro$ and has a space complexity given by $\mathcal{O}(1)$.
\end{assumption}

The bit length of objects is given by the following definition.
\begin{definition}
The bit length of an object $x$ is denoted by
\begin{equation}
    \BL(x)\,.
\end{equation}
\end{definition}
For example, the bit length of an object $x \in \cisb_n$ is given by $\BL(x) = n$.

Space-optimal implementation of $\operatorname{cipher\_map}$.

```{cpp,eval=F}
auto build_entropy_map(
    auto iterable xs,
    auto prefix_decoder d,
    auto random_oracle h)
{
  auto l = 0;
  while (true)
  {
    auto error = false;
    for (auto x : xs)
    {
      auto hash = h(l) ^ h(x);
      auto v = d(hash);
      error = (v != x) || error;
    }
    if (!error)
      break;
    l = succ(l)
  }
  return hash_map<H,D>(h,d,l);
}
```

The function `h` models the concept of a random oracle.
The function `d` models the concept of a prefix-free decoder for values in the
iterable collection `xs`.
The value of `hash` is thus an infinite random bit string that is a function of
`l` and `x`. (Note that we may model random oracles using lazy evaluation.)
Since `d` is a prefix-free decoder, then for a given `y` in `Y`, it has a
prefix-free encoding that is equal to some particular hash. 
The probability that `d(hash)` decodes `y` is thus $p(y) = 2^{-|y'|}$
where $y'$ is the encoding for $y$.
Thus, the probability that $y$ in $f($`xs`$)$ decodes successfully is given by
$$
q = \prod_{i=1}^n p(y_i) = 2^{-\sum_{i=1}^n |y_i'|}.
$$
Thus, each trial for a given $l$ has a probability of success $q$.
That is, it is geometrically distributed, and the number of trials are given
by the random variable
$$
L = \operatorname{GEO}(q),
$$
which has the expected value
$$
\frac{1}{q} = 2^{\sum_{i=1}^n |y_i'|}.
$$

Suppose $|y_i|$ are all equal to $k$, e.g., the $y$'s are uniformly distributed
and the best prefix-free coder gave them all the same length $k$.
Then, $2^{n k}$ trials are expected.

Finally, there is also an opportunity to verify that some values map correctly,
e.g., for values with the smallest encoding length, skip them, and count
on the fact that it maps correctly with some probability.
For instance, if the Bernoulli map is the set indicator function for $X$,
then $X^C$ map to `false` and we can provide a short encoding for that, say
...




\begin{figure}
    \centering
    \input{img/fig_shmap.tex}
    \caption{\emph{Singular Hash Map}}
    \label{fig:shmap}
\end{figure}

We consider a family of \emph{approximate maps} which are given by the output of the random oracle applied to the input $x \in \Mt$ concatenated with a bit string $b \in \cisb$ such that all $x \in \St$ map to the same hash. We describe the generative algorithm in \Cref{alg:ph}. Note that in \Cref{alg:shmap}, the concatenation of two bit strings $x$ and $y$ is denoted by $x \catop y$.


The value $v_i$ has a \emph{self-terminating} encoding. Thus, we need only check that the bit string that encodes $v_i$ is equal to a hash of the key (concatenated with another $1$ and another bit string $b$).


The space required for the \gls{gls-shmap} found by \Cref{alg:shmap} is of the order of the length $n$ of the bit string $b$. Therefore, for space efficiency, the algorithm exhaustively searches for a bit string in the order of increasing size $n$.

\subsection{Theoretical analysis}
The \gls{gls-set} proves that the keys in $\Mp$ are an \emph{approximate set} of the keys in $\Mt$.

\begin{theorem}
The probability that a particular bit string $b \in \cisb$ successfully codes the \gls{gls-shmap} is given by
\begin{equation}
    p(m,\varepsilon,\mu) = \frac{\varepsilon^{m-1}}{2^{m \mu}}\,.
\end{equation}
\end{theorem}
\begin{proof}
In \Cref{alg:shmap}, for a particular bit string $b_n$, the joint probability that every key in $\Mt$ collides and for each key $k$ in $\Mt$, the concatenation of $1$ and $k$ has a hash in which the first $\BL(v)$ bits collide with mapped value $v$ is given by the following reasoning.

Suppose we have a set $\Mt = \{(k_1,v_1),\ldots,(k_m,v_m)\}$ and $k_1$ hashes to $y = \hash(k_1) \mod t$, where $\hash \colon \cisb^* \mapsto \cisb_N$ is a random hash function that uniformly distributes over its domain of $2^N$ possibilities and $N < \max{\BL(v_1),\ldots,\BL(v_m)}$.

The probability that $\hash(1 \catop k_1) \mod \BL(v_1) = v_1$ is given by
\begin{equation}
    \frac{1}{2^{\BL(v_1)}}\,.
\end{equation}
Since $y$ is a particular element in $\cisb_k$, the probability that $k_j, j \neq 1$, hashes to $y$ is given by
\begin{equation}
    \frac{1}{2^k}\,.
\end{equation}
The probability that $1 \catop k_j, j \neq 1$ hashes to $v_j$ is given by
\begin{equation}
    \frac{1}{2^{\BL(v_j)}}\,.
\end{equation}
Since $\hash$ is a random hash function, every one of these probabilities are independent, and thus the joint probability is just the product given by
\begin{align}
    p &= \frac{1}{2^{k(m-1)}} \prod_{j=1}^{m} \frac{1}{2^{\BL(v_j)}}\\
      &= \varepsilon^{m-1} \frac{1}{2^{\sum_{j=1}^{m} \BL(v_j)}}\,.
\end{align}
The average bit length per key is given by
\begin{equation}
    \mu = \frac{1}{m} \sum_{j=1}^{m} \BL(v_j)
\end{equation}
and thus the probability $p$ may be parameterized as
\begin{equation}
    p = \frac{\varepsilon^{m-1}}{2^{m \mu}}\,.
\end{equation}
\end{proof}

\begin{theorem}
Given \emph{false positive} and \emph{false negative} domain error rates given by $\varepsilon$ and $\eta$ respectively, and a mean bit length $\mu$ of encodings of elements in the image of $\Afun$, the \emph{expected} bit length of the Singular Hash Map asymptotically obtains a space complexity with a lower-bound given by
\begin{equation}
    -(1 - \eta) \log_2 \varepsilon + (1 - \eta) \mu \; \si{bits \per element}\,,
\end{equation}
which occurs when the encodings have uniformly distributed bit lengths, and an upper-bound given by
\begin{equation}
    -(1 - \eta) \log_2 \varepsilon + \mu \; \si{bits \per element}\,,
\end{equation}
which occurs when the encodings have degenerate bit lengths. The greater the entropy of the bit lengths, the smaller the expected bit length given a mean of $\mu$. 
\end{theorem}
\begin{proof}
The space required for the \gls{gls-shs} found by \Cref{alg:makeset} is of the order of the length $n$ of the bit string $b_n$. Therefore, for space efficiency, the algorithm exhaustively searches for a bit string in the order of increasing size $n$.

We are interested in the first case when a success occurs, which is a geometric distribution with probability of success $p$ as given by
\begin{equation}
    \rv{Q} \sim \geodist\!\left(p = \frac{\varepsilon^{m-1}}{2^{m \mu}}\right)\,.
\end{equation}
The expected number of trials for the geometric distribution is given by
\begin{equation}
\label{eq:exp_trials}
    \expectation\left[\rv{Q}\right] = \frac{2^{m \mu}}{\varepsilon^{m-1}}\,.
\end{equation}
By \Cref{def:mapping}, the \nth trial uniquely maps to a bit string of length $m = \lfloor \log_2 n \rfloor$. Thus, the expected bit length is given approximately by
\begin{equation}
    \expectation[\log_2 \rv{Q}] = \log_2 \frac{2^{m \mu}}{\varepsilon^{m-1}}\; \si{bits}\,.
\end{equation}
Dividing by $m$ and simplifying results in
\begin{equation}
    -\frac{m-1}{m} \log_2 \varepsilon + \mu\; \si{bits \per element}\,.
\end{equation}
Asymptotically, as $m \to \infty$, the bits per element goes to
\begin{equation}
    -\log_2 \varepsilon + \mu\,.
\end{equation}
\end{proof}

The smaller the bit lengths generated by the encoder, the smaller the \emph{Singular Hash Map}. However, optimal encoders \emph{reveal} information about the values in the \emph{Singular Hash Map}, which goes against the principle of the \emph{obvlivious function}. For instance, if each input $x$ mapped to a list of numbers, then an optimal encoder will generate codes that minimize the length of the list, say a Huffman code. However, by analyzing the Huffman codes produced by the encoder, we may infer the distribution of the lists. Thus, for the sake of maintaining an \emph{oblivious} state, the encoder should generate codes as though the values were \emph{uniformly} distributed.


